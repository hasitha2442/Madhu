---
sidebar_position: 14
---
# ML


## Machine Learning

## Course Description

Upon successful completion of the course, the student will be able to:
- **CO1:** Understand the concepts of computational intelligence in machine learning. (BTL 2)
- **CO2:** Apply dimensionality reduction techniques for feature selection. (BTL 3)
- **CO3:** Apply appropriate machine learning techniques to address real-time problems. (BTL 3)
- **CO4:** Analyze ensemble models to solve classification problems. (BTL 4)

## Course Content

### Unit I: Introduction and Supervised Learning

- **Introduction to Machine Learning:**
  - Overview of Machine Learning
  - Types of Learning
  - Examples of Machine Learning Applications

- **Supervised Learning:**
  - Learning a Class from Examples
  - Probably Approximately Correct Learning
  - Learning Multiple Classes
  - Model Selection and Generalization

- **Regression:**
  - Linear Regression
  - Multiple Linear Regression
  - Logistic Regression

- **Dimensionality Reduction:**
  - Feature Selection
  - Subset Selection
  - Principal Component Analysis (PCA)
  - Linear Discriminant Analysis (LDA)

### Unit II: Decision Trees and Neural Networks

- **Decision Tree Learning:**
  - Introduction
  - Decision Tree Representation
  - Appropriate Problems for Decision Tree Learning
  - Basic Decision Tree Algorithm
  - Issues in Decision Tree Learning

- **Artificial Neural Networks:**
  - Neural Network Representation
  - Appropriate Problems for Neural Network Learning
  - Perceptrons: Gradient Descent and the Delta Rule
  - Multilayer Networks and the Backpropagation Algorithm

- **Basics of Sampling Theory:**
  - Error Estimation
  - Estimating Binomial Proportions
  - The Binomial Distribution
  - Mean and Variance
  - Estimators: Bias and Variance
  - Confidence Intervals

### Unit III: Bayesian Learning and Support Vector Machines

- **Bayesian Learning:**
  - Introduction
  - Bayes Theorem
  - Na√Øve Bayes Classifier
  - Bayes Optimal Classifier
  - Bayesian Belief Networks
  - Conditional Independence
  - Learning Bayesian Belief Networks

- **Parametric Methods:**
  - Maximum Likelihood Estimation

- **Non-parametric Methods:**
  - K Nearest Neighbor (KNN)

- **Support Vector Machine:**
  - Introduction
  - Optimal Separating Hyperplane
  - The Non-separable Case: Soft Margin Hyperplane
  - Defining Kernels

### Unit IV: Ensembles and Clustering

- **Ensembles:**
  - Introduction
  - Bagging and Boosting
  - Random Forest

- **Clustering:**
  - Introduction
  - K-means Clustering
  - Expectation Maximization Algorithm
  - Hierarchical Clustering
  - Density-based Clustering: DBSCAN
  - Choosing the Number of Clusters

- **Algorithm Evaluation Methods:**
  - Classification Accuracy
  - Confusion Matrix

## Tools and Libraries

- **Machine Learning Libraries:** Scikit-Learn, TensorFlow, Keras, PyTorch
- **Data Analysis Tools:** Pandas, NumPy

## Professor

- Dr S.vasavi

## Links

- [Lecture Notes](#)
- [Assignment Repository](#)

**Tags:** Machine Learning, Supervised Learning, Neural Networks, Bayesian Learning, Support Vector Machines, Clustering